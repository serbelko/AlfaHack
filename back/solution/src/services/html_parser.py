"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
"""
import httpx
import logging
import time
from bs4 import BeautifulSoup
from typing import Optional
from ..config import MAX_HTML_LENGTH

logger = logging.getLogger(__name__)


async def fetch_html(url: str, timeout: float = 10.0) -> Optional[str]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    
    Args:
        url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞
        
    Returns:
        HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    start_time = time.time()
    logger.debug(f"üì• –ó–∞–≥—Ä—É–∂–∞—é HTML —Å {url[:60]}...")
    
    try:
        async with httpx.AsyncClient(timeout=timeout, follow_redirects=True) as client:
            response = await client.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            })
            response.raise_for_status()
            html = response.text[:MAX_HTML_LENGTH]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            
            elapsed = time.time() - start_time
            logger.debug(f"‚úÖ HTML –∑–∞–≥—Ä—É–∂–µ–Ω ({len(html)} —Å–∏–º–≤–æ–ª–æ–≤) –∑–∞ {elapsed:.2f}—Å")
            
            return html
    except Exception as e:
        elapsed = time.time() - start_time
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ HTML —Å {url[:60]} –∑–∞ {elapsed:.2f}—Å: {e}")
        return None


def extract_text_from_html(html: str, max_length: int = 5000) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ HTML —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –¥–ª–∏–Ω–µ
    
    Args:
        html: HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∏–∑–≤–ª–µ–∫–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        
    Returns:
        –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    try:
        soup = BeautifulSoup(html, 'lxml')
        
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
        for script in soup(["script", "style", "meta", "link"]):
            script.decompose()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        text = soup.get_text(separator=' ', strip=True)
        
        # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        original_length = len(text)
        if len(text) > max_length:
            text = text[:max_length] + "..."
            logger.debug(f"üìù –¢–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω —Å {original_length} –¥–æ {max_length} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return text
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ HTML: {e}")
        return ""


async def get_text_from_url(url: str, max_length: int = 5000) -> str:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
    
    Args:
        url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
        
    Returns:
        –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    start_time = time.time()
    logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url[:60]}...")
    
    html = await fetch_html(url)
    if html:
        text = extract_text_from_html(html, max_length)
        elapsed = time.time() - start_time
        logger.info(f"‚úÖ –¢–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤) –∑–∞ {elapsed:.2f}—Å")
        return text
    
    elapsed = time.time() - start_time
    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –∑–∞ {elapsed:.2f}—Å")
    return ""

