"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AI
"""
import asyncio
import httpx
import logging
import time
from typing import List, Dict
from ..config import (
    OLLAMA_URL, MAX_TEXT_LENGTH, MAX_SEARCH_RESULTS, 
    MAX_URLS_TO_ANALYZE, MAX_TEXT_FOR_AI, PARALLEL_PARSING
)
from .google_search import search_google_async, generate_search_queries
from .html_parser import get_text_from_url

logger = logging.getLogger(__name__)


async def analyze_competitors(user_request: str) -> Dict[str, any]:
    """
    –ü—Ä–æ–≤–æ–¥–∏—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤:
    1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    2. –ò—â–µ—Ç —Å—Å—ã–ª–∫–∏ —á–µ—Ä–µ–∑ DuckDuckGo
    3. –ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    4. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —á–µ—Ä–µ–∑ AI
    
    Args:
        user_request: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    ollama_url = OLLAMA_URL.rstrip('/')
    
    # –®–∞–≥ 1: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    search_queries = await generate_search_queries(user_request, ollama_url)
    
    # –®–∞–≥ 2: –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    all_urls = []
    for query in search_queries:
        urls = await search_google_async(query, num_results=MAX_SEARCH_RESULTS)
        all_urls.extend(urls)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    unique_urls = list(dict.fromkeys(all_urls))[:MAX_URLS_TO_ANALYZE]
    
    # –®–∞–≥ 3: –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
    if PARALLEL_PARSING:
        tasks = [get_text_from_url(url, max_length=MAX_TEXT_LENGTH) for url in unique_urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        texts = []
        for url, result in zip(unique_urls, results):
            if not isinstance(result, Exception) and result:
                text_for_ai = result[:MAX_TEXT_FOR_AI] if len(result) > MAX_TEXT_FOR_AI else result
                texts.append({"url": url, "text": text_for_ai})
    else:
        texts = []
        for url in unique_urls:
            text = await get_text_from_url(url, max_length=MAX_TEXT_LENGTH)
            if text:
                text_for_ai = text[:MAX_TEXT_FOR_AI] if len(text) > MAX_TEXT_FOR_AI else text
                texts.append({"url": url, "text": text_for_ai})
    
    # –®–∞–≥ 4: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ AI (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç)
    texts_for_ai = texts[:MAX_URLS_TO_ANALYZE]
    analysis_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: {user_request}

–ò–ù–§–û–†–ú–ê–¶–ò–Ø:
{chr(10).join([f"‚Ä¢ {item['url']}: {item['text']}" for item in texts_for_ai])}

–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ:
1. –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã
2. –ò—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏  
3. –í—ã–≤–æ–¥—ã"""
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ AI
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{ollama_url}/api/generate",
            json={
                "model": "bambucha/saiga-llama3",
                "prompt": analysis_prompt,
                "stream": False
            },
            timeout=120.0
        )
        response.raise_for_status()
        
        result = response.json()
        analysis = result.get("response", "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑")
    
    return {
        "search_queries": search_queries,
        "urls_found": len(unique_urls),
        "urls_analyzed": len(texts),
        "analysis": analysis
    }


async def analyze_competitors_streaming(user_request: str):
    """
    –ü—Ä–æ–≤–æ–¥–∏—Ç –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —Å–æ streaming –æ—Ç–≤–µ—Ç–æ–º
    
    Args:
        user_request: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Yields:
        –ß–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è streaming
    """
    total_start_time = time.time()
    ollama_url = OLLAMA_URL.rstrip('/')
    
    logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{user_request[:100]}...'")
    
    # –®–∞–≥ 1: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    step_start = time.time()
    yield "üîç –ì–µ–Ω–µ—Ä–∏—Ä—É—é –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã...\n\n"
    logger.info("üìù –≠—Ç–∞–ø 1/4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    search_queries = await generate_search_queries(user_request, ollama_url)
    step_elapsed = time.time() - step_start
    yield f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(search_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–∑–∞–Ω—è–ª–æ {step_elapsed:.1f}—Å)\n\n"
    logger.info(f"‚úÖ –≠—Ç–∞–ø 1 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {step_elapsed:.2f}—Å")
    
    # –®–∞–≥ 2: –ò—â–µ–º —Å—Å—ã–ª–∫–∏
    step_start = time.time()
    yield "üåê –ò—â—É —Å—Å—ã–ª–∫–∏ —á–µ—Ä–µ–∑ DuckDuckGo...\n\n"
    logger.info("üìù –≠—Ç–∞–ø 2/4: –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ DuckDuckGo")
    
    all_urls = []
    for i, query in enumerate(search_queries, 1):
        yield f"  –ü–æ–∏—Å–∫ {i}/{len(search_queries)}: {query}\n"
        query_start = time.time()
        try:
            urls = await search_google_async(query, num_results=MAX_SEARCH_RESULTS)
            query_elapsed = time.time() - query_start
            all_urls.extend(urls)
            if urls:
                yield f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(urls)} —Å—Å—ã–ª–æ–∫ ({query_elapsed:.1f}—Å)\n"
            else:
                yield f"  ‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ ({query_elapsed:.1f}—Å) - –≤–æ–∑–º–æ–∂–Ω–æ, Google –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã\n"
        except Exception as e:
            query_elapsed = time.time() - query_start
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ '{query}': {e}")
            yield f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ ({query_elapsed:.1f}—Å): {str(e)[:50]}\n"
    
    unique_urls = list(dict.fromkeys(all_urls))[:MAX_URLS_TO_ANALYZE]
    step_elapsed = time.time() - step_start
    yield f"\nüìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(unique_urls)} (–±—É–¥–µ—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –º–∞–∫—Å–∏–º—É–º {MAX_URLS_TO_ANALYZE}) (–ø–æ–∏—Å–∫ –∑–∞–Ω—è–ª {step_elapsed:.1f}—Å)\n\n"
    logger.info(f"‚úÖ –≠—Ç–∞–ø 2 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {step_elapsed:.2f}—Å, –Ω–∞–π–¥–µ–Ω–æ {len(unique_urls)} —Å—Å—ã–ª–æ–∫")
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫, –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if not unique_urls:
        yield "‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫–∏ —á–µ—Ä–µ–∑ DuckDuckGo.\n"
        yield "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
        yield "  ‚Ä¢ –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç—å—é –∏–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º\n"
        yield "  ‚Ä¢ –ó–∞–ø—Ä–æ—Å –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n"
        yield "  ‚Ä¢ –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π\n\n"
        yield "–ü—Ä–æ–¥–æ–ª–∂–∞—é –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏...\n\n"
    
    # –®–∞–≥ 3: –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)
    step_start = time.time()
    yield "üìÑ –ü–æ–ª—É—á–∞—é —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü...\n\n"
    logger.info(f"üìù –≠—Ç–∞–ø 3/4: –ü–∞—Ä—Å–∏–Ω–≥ {len(unique_urls)} —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ: {PARALLEL_PARSING})")
    
    texts = []
    
    if PARALLEL_PARSING:
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        yield f"  ‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(unique_urls)} —Å—Ç—Ä–∞–Ω–∏—Ü...\n"
        tasks = [get_text_from_url(url, max_length=MAX_TEXT_LENGTH) for url in unique_urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, (url, result) in enumerate(zip(unique_urls, results), 1):
            if isinstance(result, Exception):
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {result}")
                yield f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {i}/{len(unique_urls)}: {url[:50]}...\n"
            elif result:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è AI
                text_for_ai = result[:MAX_TEXT_FOR_AI] if len(result) > MAX_TEXT_FOR_AI else result
                texts.append({"url": url, "text": text_for_ai})
                yield f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(unique_urls)}: {len(text_for_ai)} —Å–∏–º–≤–æ–ª–æ–≤\n"
            else:
                yield f"  ‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç {i}/{len(unique_urls)}: {url[:50]}...\n"
    else:
        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        for i, url in enumerate(unique_urls, 1):
            yield f"  –û–±—Ä–∞–±–æ—Ç–∫–∞ {i}/{len(unique_urls)}: {url[:50]}...\n"
            url_start = time.time()
            text = await get_text_from_url(url, max_length=MAX_TEXT_LENGTH)
            url_elapsed = time.time() - url_start
            if text:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è AI
                text_for_ai = text[:MAX_TEXT_FOR_AI] if len(text) > MAX_TEXT_FOR_AI else text
                texts.append({"url": url, "text": text_for_ai})
                yield f"  ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(text_for_ai)} —Å–∏–º–≤–æ–ª–æ–≤ ({url_elapsed:.1f}—Å)\n"
            else:
                yield f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç ({url_elapsed:.1f}—Å)\n"
    
    step_elapsed = time.time() - step_start
    yield f"\nüìö –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(texts)} —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ {len(unique_urls)} (–ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–Ω—è–ª {step_elapsed:.1f}—Å)\n\n"
    logger.info(f"‚úÖ –≠—Ç–∞–ø 3 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {step_elapsed:.2f}—Å, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(texts)} —Å—Ç—Ä–∞–Ω–∏—Ü")
    
    # –®–∞–≥ 4: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ AI —Å–æ streaming
    step_start = time.time()
    yield "ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —á–µ—Ä–µ–∑ AI...\n\n"
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    texts_for_ai = texts[:MAX_URLS_TO_ANALYZE]
    total_chars = sum(len(t['text']) for t in texts_for_ai)
    logger.info(f"üìù –≠—Ç–∞–ø 4/4: AI-–∞–Ω–∞–ª–∏–∑ ({len(texts_for_ai)} —Å—Ç—Ä–∞–Ω–∏—Ü, ~{total_chars} —Å–∏–º–≤–æ–ª–æ–≤)")
    
    # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if not texts_for_ai:
        yield "‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü).\n"
        yield "–ü—Ä–æ–≤–æ–∂—É –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–µ–≥–æ –∑–Ω–∞–Ω–∏—è –æ —Ä—ã–Ω–∫–µ...\n\n"
        analysis_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤ —Å—Ñ–µ—Ä–µ: {user_request}

–ù–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–µ–≥–æ –∑–Ω–∞–Ω–∏—è –æ —Ä—ã–Ω–∫–µ –æ—Ç–≤–µ—Ç—å:
1. –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –≤ —ç—Ç–æ–π —Å—Ñ–µ—Ä–µ
2. –ò—Ö —Ç–∏–ø–∏—á–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
3. –û–±—â–∏–µ –≤—ã–≤–æ–¥—ã –æ —Ä—ã–Ω–∫–µ

–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º, –¥–∞–∂–µ –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    else:
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç - –∫–æ—Ä–æ—á–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        analysis_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: {user_request}

–ò–ù–§–û–†–ú–ê–¶–ò–Ø:
{chr(10).join([f"‚Ä¢ {item['url']}: {item['text']}" for item in texts_for_ai])}

–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ:
1. –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã
2. –ò—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏  
3. –í—ã–≤–æ–¥—ã"""
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º streaming –∑–∞–ø—Ä–æ—Å –≤ AI
    try:
        async with httpx.AsyncClient() as client:
            async with client.stream(
                "POST",
                f"{ollama_url}/api/chat",
                json={
                    "model": "bambucha/saiga-llama3",
                    "messages": [{"role": "user", "content": analysis_prompt}],
                    "stream": True
                },
                timeout=120.0
            ) as response:
                response.raise_for_status()
                logger.info("‚úÖ AI –Ω–∞—á–∞–ª –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç (streaming)")
                chunk_count = 0
                async for line in response.aiter_lines():
                    if line.strip():
                        try:
                            import json
                            chunk = json.loads(line)
                            if "message" in chunk and "content" in chunk["message"]:
                                content = chunk["message"]["content"]
                                if content:
                                    yield content
                                    chunk_count += 1
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ chunk: {e}")
                            continue
                
                step_elapsed = time.time() - step_start
                logger.info(f"‚úÖ –≠—Ç–∞–ø 4 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {step_elapsed:.2f}—Å, –ø–æ–ª—É—á–µ–Ω–æ {chunk_count} chunks")
    except Exception as e:
        step_elapsed = time.time() - step_start
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ AI-–∞–Ω–∞–ª–∏–∑–µ –∑–∞ {step_elapsed:.2f}—Å: {e}")
        yield f"\n\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —á–µ—Ä–µ–∑ AI: {str(e)}\n"
        raise
    
    total_elapsed = time.time() - total_start_time
    logger.info(f"üéâ –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {total_elapsed:.2f}—Å ({total_elapsed/60:.1f} –º–∏–Ω—É—Ç)")
    yield f"\n\n‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {total_elapsed:.1f}—Å ({total_elapsed/60:.1f} –º–∏–Ω—É—Ç)\n"

