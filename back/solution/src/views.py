import json
import asyncio
from src.utils import parse_model_response
import httpx
<<<<<<< Updated upstream
import logging
from fastapi.responses import StreamingResponse
from .config import OLLAMA_URL
=======
from fastapi.responses import Response, StreamingResponse
from .config import OLLAMA_URL, API_BASE_URL
>>>>>>> Stashed changes
from .schemas import PromptRequest
from .services.competitor_analyzer import analyze_competitors_streaming

from ollama import Client
ollama_client = Client(
  host=OLLAMA_URL,
)

<<<<<<< Updated upstream
logger = logging.getLogger(__name__)
=======
async def generate_stream_generator(response: httpx.Response) -> str:
    response.raise_for_status()
    async for line in response.aiter_lines():
        if line.strip():
            try:
                chunk = json.loads(line)

                if "message" in chunk and "content" in chunk["message"]:
                    content = chunk["message"]["content"]
                    if content:
                        yield content
                        # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —á–∞–Ω–∫–æ–≤
                        await asyncio.sleep(0)

            except json.JSONDecodeError as e:
                print(f"JSON decode error: {e}, line: {line[:100]}")
                continue

async def responce_stream(response: httpx.Response) -> StreamingResponse:
    return StreamingResponse(
        generate_stream_generator(response),
        media_type="text/plain; charset=utf-8",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Pragma": "no-cache",
            "Expires": "0",
            "Transfer-Encoding": "chunked"
        }
    )
>>>>>>> Stashed changes

async def get_ai_message_mock(payload: PromptRequest):
    async def stream_generator():
        async with httpx.AsyncClient(timeout=httpx.Timeout(120.0, connect=10.0)) as client:
            async with client.stream(
                "POST",
                f"{OLLAMA_URL.rstrip('/')}/api/chat",
                json={
                    "model": "bambucha/saiga-llama3",
                    "messages": [{"role": "user", "content": "–ü–æ–∑–¥–æ—Ä–æ–≤–∞–π—Å—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–µ–∂–ª–∏–≤–æ –∏ –ø–æ–ø—Ä–æ—Å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–≤–µ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å"}],
                    "stream": True
                }
            ) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if line.strip():
                        try:
                            chunk = json.loads(line)
                            if "message" in chunk and "content" in chunk["message"]:
                                content = chunk["message"]["content"]
                                if content:
                                    yield content
                                    await asyncio.sleep(0)
                        except json.JSONDecodeError as e:
                            print(f"JSON decode error: {e}, line: {line[:100]}")
                            continue
    
    return StreamingResponse(
        stream_generator(),
        media_type="text/plain; charset=utf-8",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Pragma": "no-cache",
            "Expires": "0",
            "Transfer-Encoding": "chunked"
        }
    )
    


async def fetch_api_data(endpoints: list[str]) -> list[dict]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º API endpoints.
    
    Args:
        endpoints: –°–ø–∏—Å–æ–∫ endpoints –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö –∏–∑ API
    """
    if not isinstance(endpoints, list) or len(endpoints) == 0:
        return []
    
    # –î–µ–ª–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫–æ –≤—Å–µ–º endpoints
    async with httpx.AsyncClient() as api_client:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        async def fetch_endpoint(endpoint: str):
            """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ –æ–¥–Ω–æ–º—É endpoint"""
            try:
                response = await api_client.get(
                    f"{API_BASE_URL}{endpoint}",
                    timeout=30.0
                )
                response.raise_for_status()
                return {"endpoint": endpoint, "data": response.json(), "success": True}
            except Exception as e:
                return {"endpoint": endpoint, "error": str(e), "success": False}
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = [fetch_endpoint(endpoint) for endpoint in endpoints]
        results = await asyncio.gather(*tasks)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        all_api_data = []
        for result in results:
            if result["success"]:
                all_api_data.append({
                    "endpoint": result["endpoint"],
                    "data": result["data"]
                })
            else:
                all_api_data.append({
                    "endpoint": result["endpoint"],
                    "error": result["error"]
                })

        return all_api_data

async def receive_final_prompt(all_api_data: list[dict], user_prompt: str) -> StreamingResponse:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ API –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –µ–≥–æ –≤ –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞.
    
    Args:
        all_api_data: –î–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö API –∑–∞–ø—Ä–æ—Å–æ–≤
        user_prompt: –ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Returns:
        StreamingResponse —Å –æ—Ç–≤–µ—Ç–æ–º –æ—Ç –º–æ–¥–µ–ª–∏
    """
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –≤—Å–µ—Ö API –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –ø–µ—Ä–µ–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –º–æ–¥–µ–ª—å
    final_prompt = f"""
    –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–ª –≤–æ–ø—Ä–æ—Å: {user_prompt}
    
    –î–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –±—ã–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∫ API:
    {json.dumps(all_api_data, ensure_ascii=False, indent=2)}
    
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –¥–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
    –ï—Å–ª–∏ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –±—ã–ª–∏ –æ—à–∏–±–∫–∏, —É—á—Ç–∏ —ç—Ç–æ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞.
    """
    
    # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Å–∞–º —É–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Å—Ç—Ä–∏–º–∞
    async def stream_generator():
        async with httpx.AsyncClient(timeout=httpx.Timeout(120.0, connect=10.0)) as client:
            async with client.stream(
                "POST",
                f"{OLLAMA_URL.rstrip('/')}/api/chat",
                json={
                    "model": "bambucha/saiga-llama3",
                    "messages": [{"role": "user", "content": final_prompt}],
                    "stream": True
                }
            ) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if line.strip():
                        try:
                            chunk = json.loads(line)
                            if "message" in chunk and "content" in chunk["message"]:
                                content = chunk["message"]["content"]
                                if content:
                                    yield content
                                    await asyncio.sleep(0)
                        except json.JSONDecodeError as e:
                            print(f"JSON decode error: {e}, line: {line[:100]}")
                            continue
    
    return StreamingResponse(
        stream_generator(),
        media_type="text/plain; charset=utf-8",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Pragma": "no-cache",
            "Expires": "0",
            "Transfer-Encoding": "chunked"
        }
    )


<<<<<<< Updated upstream
async def get_ai_message(payload: PromptRequest):
=======

async def get_requests(payload: PromptRequest) -> str:
    system_prompt = f"""
        –¢—ã - –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—è —Å –≤–Ω–µ—à–Ω–∏–º API.
        –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Ä–µ—à–∞—Ç—å, –ø–æ –∫–∞–∫–∏–º –º–µ—Ç–æ–¥–∞–º –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ API.

        –î–æ—Å—Ç—É–ø–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞:
        1. GET /api/amount/ - –¥–∞–Ω–Ω—ã–µ –ø–æ —Å—á–µ—Ç—É
        –ü–æ–ª—è: name, count
        2. GET /api/amount/transaction - –¥–∞–Ω–Ω—ã–µ –æ–± –æ–¥–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        –ü–æ–ª—è: amount_id, created_at, type, category, count
        3. GET /api/amount/history - –∏—Å—Ç–æ—Ä–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
        –ü–æ–ª—è: amount_id, created_at, type, category, count

        —ç–Ω–¥–ø–æ–∏–Ω—Ç history –∏–º–µ–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: amount_id, from_date, to_date, type, category, count
        –ü–æ –Ω–∏–º –º–æ–∂–Ω–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Ç–∞–∫: 
        /api/amount/history?amount_id=1&from_date=2024-01-01&to_date=2024-01-31&type=income&category=salary

        –ï—Å–ª–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Ç—Ä–µ–±—É–µ—Ç—Å—è –≤—ã–∑–æ–≤ API, —Ç—ã –î–û–õ–ñ–ï–ù –≤–µ—Ä–Ω—É—Ç—å –æ—Ç–≤–µ—Ç –≤ —Å—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É—é—â–µ–º JSON-—Ñ–æ—Ä–º–∞—Ç–µ:
        {{
            "endpoints": ["/–∑–¥–µ—Å—å_–Ω—É–∂–Ω—ã–π_—ç–Ω–¥–ø–æ–∏–Ω—Ç1", "/–∑–¥–µ—Å—å_–Ω—É–∂–Ω—ã–π_—ç–Ω–¥–ø–æ–∏–Ω—Ç2", ...]
        }}
        
        –ï—Å–ª–∏ –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω endpoint, –∏—Å–ø–æ–ª—å–∑—É–π –º–∞—Å—Å–∏–≤ —Å –æ–¥–Ω–∏–º —ç–ª–µ–º–µ–Ω—Ç–æ–º: {{"endpoints": ["/api/amount/"]}}
        –ï—Å–ª–∏ –≤—ã–∑–æ–≤ API –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, –æ—Ç–≤–µ—Ç—å –∫–∞–∫ –æ–±—ã—á–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫.

        –¢–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {payload.prompt}
        """

    async with httpx.AsyncClient(timeout=httpx.Timeout(120.0, connect=10.0)) as client:
        try:
            response = await client.post(
                f"{OLLAMA_URL.rstrip('/')}/api/generate",
                json={
                    "model": "bambucha/saiga-llama3",
                    "prompt": system_prompt,
                    "stream": False
                }
            )
            response.raise_for_status()
            response_data = response.json()
            if action_data := parse_model_response(response_data.get("response", "")):
                return action_data
            else:
                return Response(status_code=200, content=response_data.get("response", ""))
        except httpx.ReadTimeout:
            return Response(status_code=504, content="–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ –º–æ–¥–µ–ª–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
        except httpx.HTTPError as e:
            return Response(status_code=500, content=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ –º–æ–¥–µ–ª–∏: {str(e)}")





async def get_ai_message(payload: PromptRequest) -> str | StreamingResponse:
>>>>>>> Stashed changes
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI.
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.
    
    Args:
        payload: –ó–∞–ø—Ä–æ—Å —Å –ø—Ä–æ–º–ø—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Returns:
<<<<<<< Updated upstream
        –î–ª—è MRKT - StreamingResponse —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        –î–ª—è FIN - —Å—Ç—Ä–æ–∫–∞ "FIN"
=======
        –û—Ç–≤–µ—Ç –æ—Ç AI –º–æ–¥–µ–ª–∏ (—Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ StreamingResponse)
>>>>>>> Stashed changes
    """
    logger.info(f"üì• –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é: '{payload.prompt[:100]}...'")
    
    classification_prompt = """
        ROLE: –¢—ã ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –¢—ã –Ω–µ –¥–∞–µ—à—å –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∞ —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—à—å –∏—Ö.
        TASK: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ "INPUT:" –∏ –≤–µ—Ä–Ω–∏ —Ä–æ–≤–Ω–æ –æ–¥–∏–Ω –∏–∑ –¥–≤—É—Ö —Ç–µ–≥–æ–≤: `[FIN]` –∏–ª–∏ `[MRKT]`.
        CRITERIA:
        - `[FIN]` (Finance): –í–æ–ø—Ä–æ—Å—ã –æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–Ω–µ–∂–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö, –±—é–¥–∂–µ—Ç–µ, –ø—Ä–∏–±—ã–ª–∏, –∑–∞—Ç—Ä–∞—Ç–∞—Ö, –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–∞–Ω–∏–∏.
        - `[MRKT]` (Market): –í–æ–ø—Ä–æ—Å—ã –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö, –¥–æ–ª–µ —Ä—ã–Ω–∫–∞, —Ç—Ä–µ–Ω–¥–∞—Ö, –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è—Ö, —Å–ø—Ä–æ—Å–µ.
        INSTRUCTION: –ù–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–π, –Ω–µ –∏–∑–≤–∏–Ω—è–π—Å—è, –Ω–µ –æ–±—ä—è—Å–Ω—è–π —Å–≤–æ–π –≤—ã–±–æ—Ä. –¢–æ–ª—å–∫–æ —Ç–µ–≥.
        OUTPUT_FORMAT: [FIN] –∏–ª–∏ [MRKT]

        INPUT: {prompt}
    """.format(prompt=payload.prompt)

<<<<<<< Updated upstream
    try:
        async with httpx.AsyncClient() as client:
            logger.debug("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –≤ AI")
=======
    async with httpx.AsyncClient(timeout=httpx.Timeout(120.0, connect=10.0)) as client:
        try:
>>>>>>> Stashed changes
            response = await client.post(
                f"{OLLAMA_URL.rstrip('/')}/api/generate",
                json={
                    "model": "bambucha/saiga-llama3",
                    "prompt": classification_prompt,
                    "stream": False
<<<<<<< Updated upstream
                },
                timeout=30.0
=======
                }
>>>>>>> Stashed changes
            )
            response.raise_for_status()

            response_data = response.json()
<<<<<<< Updated upstream
            category = response_data.get("response", "").strip()
            logger.info(f"‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: {category}")
            
            if "[MRKT]" in category or category == "MRKT":
                # –î–ª—è MRKT –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —Å–æ streaming
                logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞—é –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è MRKT –∑–∞–ø—Ä–æ—Å–∞")
                return await analyze_competitors(payload)
            elif "[FIN]" in category or category == "FIN":
                # –î–ª—è FIN –∑–∞–ø—Ä–æ—Å–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                logger.info("‚úÖ –ó–∞–ø—Ä–æ—Å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ FIN")
                return {"category": "FIN", "message": "–ó–∞–ø—Ä–æ—Å –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Finance"}
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é. –û—Ç–≤–µ—Ç AI: {category}")
                return {"category": "UNKNOWN", "message": f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é. –û—Ç–≤–µ—Ç AI: {category}"}
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}", exc_info=True)
        return {"category": "ERROR", "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {str(e)}"}


async def analyze_competitors(payload: PromptRequest):
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —Å–æ streaming –æ—Ç–≤–µ—Ç–æ–º
    
    Args:
        payload: –ó–∞–ø—Ä–æ—Å —Å –ø—Ä–æ–º–ø—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Returns:
        StreamingResponse —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    logger.info(f"üì• –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: '{payload.prompt[:100]}...'")
    
    async def generate():
        try:
            async for chunk in analyze_competitors_streaming(payload.prompt):
                yield chunk
                await asyncio.sleep(0)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —á–∞–Ω–∫–æ–≤
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}", exc_info=True)
            yield f"\n\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/plain; charset=utf-8",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Pragma": "no-cache",
            "Expires": "0",
            "Transfer-Encoding": "chunked"
        }
    )
=======
            classification_result = response_data.get("response", "").strip()
            if classification_result == "[FIN]":
                requests_data = await get_requests(payload)
                api_data = await fetch_api_data(requests_data["endpoints"])
                return await receive_final_prompt(api_data, payload.prompt)

            elif classification_result == "[MRKT]":
                return "MRKT"
            else:
                return f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {classification_result}"
        except httpx.ReadTimeout:
            return "–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        except httpx.HTTPError as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"
>>>>>>> Stashed changes

