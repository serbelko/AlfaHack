import asyncio
import json
import logging
from typing import Any

import httpx
from fastapi import Request
from fastapi.responses import Response, StreamingResponse

from src.utils import parse_model_response
from .config import API_BASE_URL, OLLAMA_URL, SERVICE_API_TOKEN
from .schemas import PromptRequest
from .services.competitor_analyzer import analyze_competitors_streaming

from ollama import Client

ollama_client = Client(host=OLLAMA_URL)
logger = logging.getLogger(__name__)


async def get_ai_message_mock(payload: PromptRequest):
    async def stream_generator():
        async with httpx.AsyncClient(timeout=httpx.Timeout(120.0, connect=10.0)) as client:
            async with client.stream(
                "POST",
                f"{OLLAMA_URL.rstrip('/')}/api/chat",
                json={
                    "model": "bambucha/saiga-llama3",
                    "messages": [{"role": "user", "content": "–ü–æ–∑–¥–æ—Ä–æ–≤–∞–π—Å—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–µ–∂–ª–∏–≤–æ –∏ –ø–æ–ø—Ä–æ—Å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–≤–µ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å"}],
                    "stream": True,
                },
            ) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if not line.strip():
                        continue
                    try:
                        chunk = json.loads(line)
                        message = chunk.get("message", {})
                        content = message.get("content")
                        if content:
                            yield content
                            await asyncio.sleep(0)
                    except json.JSONDecodeError as exc:
                        logger.debug("JSON decode error: %s", exc)
                        continue

    return StreamingResponse(
        stream_generator(),
        media_type="text/plain; charset=utf-8",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Pragma": "no-cache",
            "Expires": "0",
            "Transfer-Encoding": "chunked",
        },
    )


async def fetch_api_data(endpoints: list[str], authorization_header: str | None = None) -> list[dict[str, Any]]:
    if not endpoints:
        return []

    headers = {}

    if authorization_header:
        headers["Authorization"] = authorization_header

    async with httpx.AsyncClient() as api_client:
        async def fetch_endpoint(endpoint: str):
            try:
                response = await api_client.get(
                    f"{API_BASE_URL}{endpoint}",
                    headers=headers,
                    timeout=30.0
                )
                response.raise_for_status()
                return {"endpoint": endpoint, "data": response.json(), "success": True}
            except Exception as exc:
d                return {"endpoint": endpoint, "error": str(exc), "success": False}

        results = await asyncio.gather(*(fetch_endpoint(endpoint) for endpoint in endpoints))

    aggregated: list[dict[str, Any]] = []
    for result in results:
        if result["success"]:
            aggregated.append({"endpoint": result["endpoint"], "data": result["data"]})
        else:
            aggregated.append({"endpoint": result["endpoint"], "error": result["error"]})
    return aggregated


async def receive_final_prompt(all_api_data: list[dict[str, Any]], user_prompt: str) -> StreamingResponse:
    final_prompt = f"""
    –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–ª –≤–æ–ø—Ä–æ—Å: {user_prompt}

    –î–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –±—ã–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∫ API:
    {json.dumps(all_api_data, ensure_ascii=False, indent=2)}

    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –¥–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    –ï—Å–ª–∏ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –±—ã–ª–∏ –æ—à–∏–±–∫–∏, —É—á—Ç–∏ —ç—Ç–æ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞.
    """

    async def stream_generator():
        async with httpx.AsyncClient(timeout=httpx.Timeout(120.0, connect=10.0)) as client:
            async with client.stream(
                "POST",
                f"{OLLAMA_URL.rstrip('/')}/api/chat",
                json={
                    "model": "bambucha/saiga-llama3",
                    "messages": [{"role": "user", "content": final_prompt}],
                    "stream": True,
                },
            ) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if not line.strip():
                        continue
                    try:
                        chunk = json.loads(line)
                        message = chunk.get("message", {})
                        content = message.get("content")
                        if content:
                            yield content
                            await asyncio.sleep(0)
                    except json.JSONDecodeError as exc:
                        logger.debug("JSON decode error: %s", exc)
                        continue

    return StreamingResponse(
        stream_generator(),
        media_type="text/plain; charset=utf-8",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Pragma": "no-cache",
            "Expires": "0",
            "Transfer-Encoding": "chunked",
        },
    )


async def get_requests(payload: PromptRequest) -> dict[str, list[str]] | Response:
    system_prompt = f"""
        –¢—ã - –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—è —Å –≤–Ω–µ—à–Ω–∏–º API.
        –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Ä–µ—à–∞—Ç—å, –∫–∞–∫–∏–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –≤—ã–∑–≤–∞—Ç—å.

        –î–æ—Å—Ç—É–ø–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã:
        1. GET /api/amount/ - –¥–∞–Ω–Ω—ã–µ –ø–æ —Å—á–µ—Ç—É (name, count)
        2. GET /api/amount/transaction - –¥–∞–Ω–Ω—ã–µ –æ–± –æ–¥–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (amount_id, created_at, type, category, count)
        3. GET /api/amount/history - –∏—Å—Ç–æ—Ä–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π (amount_id, created_at, type, category, count)

        –≠–Ω–¥–ø–æ–∏–Ω—Ç history –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã amount_id, from_date, to_date, type, category, count.
        –ü—Ä–∏–º–µ—Ä: /api/amount/history?amount_id=1&from_date=2024-01-01&to_date=2024-01-31&type=income

        –í–µ—Ä–Ω–∏ JSON —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
        {{
            "endpoints": ["/–∑–¥–µ—Å—å_–Ω—É–∂–Ω—ã–π_—ç–Ω–¥–ø–æ–∏–Ω—Ç1", "/–∑–¥–µ—Å—å_–Ω—É–∂–Ω—ã–π_—ç–Ω–¥–ø–æ–∏–Ω—Ç2", ...]
        }}

        –ï—Å–ª–∏ –≤—ã–∑–æ–≤ API –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, –æ—Ç–≤–µ—Ç—å –∫–∞–∫ –æ–±—ã—á–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫.

        –¢–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {payload.prompt}
    """

    async with httpx.AsyncClient(timeout=httpx.Timeout(120.0, connect=10.0)) as client:
        try:
            response = await client.post(
                f"{OLLAMA_URL.rstrip('/')}/api/generate",
                json={
                    "model": "bambucha/saiga-llama3",
                    "prompt": system_prompt,
                    "stream": False,
                },
            )
            response.raise_for_status()
            response_data = response.json()

            action_data = parse_model_response(response_data.get("response", ""))
            if action_data:
                return action_data

            return Response(status_code=200, content=response_data.get("response", ""))
        except httpx.ReadTimeout:
            return Response(status_code=504, content="–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ –º–æ–¥–µ–ª–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
        except httpx.HTTPError as exc:
            return Response(status_code=500, content=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ –º–æ–¥–µ–ª–∏: {str(exc)}")


async def get_ai_message(payload: PromptRequest, request: Request) -> Response | StreamingResponse:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å: –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –µ–≥–æ –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Å—Ü–µ–Ω–∞—Ä–∏–π.
    """
    logger.info("üì• –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é: '%s...'", payload.prompt[:100])
    
    # –ü—Ä–∏–Ω–∏–º–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ Authorization –∫–∞–∫ –µ—Å—Ç—å
    authorization_header = request.headers.get("Authorization")

    classification_prompt = f"""
        ROLE: –¢—ã ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –¢—ã –Ω–µ –¥–∞–µ—à—å –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∞ —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—à—å –∏—Ö.
        TASK: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ "INPUT:" –∏ –≤–µ—Ä–Ω–∏ —Ä–æ–≤–Ω–æ –æ–¥–∏–Ω –∏–∑ –¥–≤—É—Ö —Ç–µ–≥–æ–≤: `[FIN]` –∏–ª–∏ `[MRKT]`.
        CRITERIA:
        - `[FIN]` (Finance): –í–æ–ø—Ä–æ—Å—ã –æ –¥–µ–Ω–µ–∂–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö, –±—é–¥–∂–µ—Ç–µ, –ø—Ä–∏–±—ã–ª–∏, –∑–∞—Ç—Ä–∞—Ç–∞—Ö, –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–∞–Ω–∏–∏.
        - `[MRKT]` (Market): –í–æ–ø—Ä–æ—Å—ã –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö, –¥–æ–ª–µ —Ä—ã–Ω–∫–∞, —Ç—Ä–µ–Ω–¥–∞—Ö, –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è—Ö, —Å–ø—Ä–æ—Å–µ.
        OUTPUT_FORMAT: [FIN] –∏–ª–∏ [MRKT]

        INPUT: {payload.prompt}
    """

    async with httpx.AsyncClient(timeout=httpx.Timeout(120.0, connect=10.0)) as client:
        try:
            response = await client.post(
                f"{OLLAMA_URL.rstrip('/')}/api/generate",
                json={
                    "model": "bambucha/saiga-llama3",
                    "prompt": classification_prompt,
                    "stream": False,
                },
            )
            response.raise_for_status()

            classification_result = response.json().get("response", "").strip()
            logger.info("‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: %s", classification_result)

            if classification_result == "[FIN]":
                requests_data = await get_requests(payload)
                if isinstance(requests_data, Response):
                    return requests_data
                endpoints = requests_data.get("endpoints", [])
                api_data = await fetch_api_data(endpoints, authorization_header=authorization_header)
                return await receive_final_prompt(api_data, payload.prompt)

            if classification_result == "[MRKT]":
                return await analyze_competitors(payload)

            logger.warning("‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: %s", classification_result)
            return Response(status_code=400, content=f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞: {classification_result}")
        except httpx.ReadTimeout:
            logger.error("‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞")
            return Response(status_code=504, content="–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
        except httpx.HTTPError as exc:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: %s", exc)
            return Response(status_code=500, content=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(exc)}")


async def analyze_competitors(payload: PromptRequest) -> StreamingResponse:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç streaming-–∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.
    """
    logger.info("üì• –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: '%s...'", payload.prompt[:100])

    async def generate():
        try:
            async for chunk in analyze_competitors_streaming(payload.prompt):
                yield chunk
                await asyncio.sleep(0)
        except Exception as exc:
            logger.error("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: %s", exc, exc_info=True)
            yield f"\n\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(exc)}\n"

    return StreamingResponse(
        generate(),
        media_type="text/plain; charset=utf-8",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Pragma": "no-cache",
            "Expires": "0",
            "Transfer-Encoding": "chunked",
        },
    )

