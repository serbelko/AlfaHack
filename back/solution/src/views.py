import json
import asyncio
import httpx
import logging
from fastapi.responses import StreamingResponse
from .config import OLLAMA_URL
from .schemas import PromptRequest
from .services.competitor_analyzer import analyze_competitors_streaming

from ollama import Client
ollama_client = Client(
  host=OLLAMA_URL,
)

logger = logging.getLogger(__name__)

async def get_ai_message_mock(payload: PromptRequest):
    async def generate():
        async with httpx.AsyncClient() as client:
            async with client.stream(
                "POST",
                f"{OLLAMA_URL.rstrip('/')}/api/chat",
                json={
                    "model": "bambucha/saiga-llama3",
                    "messages": [{"role": "user", "content": "–ü–æ–∑–¥–æ—Ä–æ–≤–∞–π—Å—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–µ–∂–ª–∏–≤–æ –∏ –ø–æ–ø—Ä–æ—Å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–≤–µ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å"}],
                    "stream": True
                },
                timeout=30.0
            ) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if line.strip():
                        try:
                            chunk = json.loads(line)

                            if "message" in chunk and "content" in chunk["message"]:
                                content = chunk["message"]["content"]
                                if content:
                                    yield content
                                    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —á–∞–Ω–∫–æ–≤
                                    await asyncio.sleep(0)

                        except json.JSONDecodeError as e:
                            print(f"JSON decode error: {e}, line: {line[:100]}")
                            continue
    
    return StreamingResponse(
        generate(), 
        media_type="text/plain; charset=utf-8",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Pragma": "no-cache",
            "Expires": "0",
            "Transfer-Encoding": "chunked"
        }
    )


async def get_ai_message(payload: PromptRequest):
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI.
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.
    
    Args:
        payload: –ó–∞–ø—Ä–æ—Å —Å –ø—Ä–æ–º–ø—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Returns:
        –î–ª—è MRKT - StreamingResponse —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        –î–ª—è FIN - —Å—Ç—Ä–æ–∫–∞ "FIN"
    """
    logger.info(f"üì• –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é: '{payload.prompt[:100]}...'")
    
    classification_prompt = """
        ROLE: –¢—ã ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –¢—ã –Ω–µ –¥–∞–µ—à—å –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∞ —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—à—å –∏—Ö.
        TASK: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ "INPUT:" –∏ –≤–µ—Ä–Ω–∏ —Ä–æ–≤–Ω–æ –æ–¥–∏–Ω –∏–∑ –¥–≤—É—Ö —Ç–µ–≥–æ–≤: `[FIN]` –∏–ª–∏ `[MRKT]`.
        CRITERIA:
        - `[FIN]` (Finance): –í–æ–ø—Ä–æ—Å—ã –æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–Ω–µ–∂–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö, –±—é–¥–∂–µ—Ç–µ, –ø—Ä–∏–±—ã–ª–∏, –∑–∞—Ç—Ä–∞—Ç–∞—Ö, –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–∞–Ω–∏–∏.
        - `[MRKT]` (Market): –í–æ–ø—Ä–æ—Å—ã –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö, –¥–æ–ª–µ —Ä—ã–Ω–∫–∞, —Ç—Ä–µ–Ω–¥–∞—Ö, –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è—Ö, —Å–ø—Ä–æ—Å–µ.
        INSTRUCTION: –ù–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–π, –Ω–µ –∏–∑–≤–∏–Ω—è–π—Å—è, –Ω–µ –æ–±—ä—è—Å–Ω—è–π —Å–≤–æ–π –≤—ã–±–æ—Ä. –¢–æ–ª—å–∫–æ —Ç–µ–≥.
        OUTPUT_FORMAT: [FIN] –∏–ª–∏ [MRKT]

        INPUT: {prompt}
    """.format(prompt=payload.prompt)

    try:
        async with httpx.AsyncClient() as client:
            logger.debug("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –≤ AI")
            response = await client.post(
                f"{OLLAMA_URL.rstrip('/')}/api/generate",
                json={
                    "model": "bambucha/saiga-llama3",
                    "prompt": classification_prompt,
                    "stream": False
                },
                timeout=30.0
            )
            response.raise_for_status()

            response_data = response.json()
            category = response_data.get("response", "").strip()
            logger.info(f"‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: {category}")
            
            if "[MRKT]" in category or category == "MRKT":
                # –î–ª—è MRKT –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —Å–æ streaming
                logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞—é –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è MRKT –∑–∞–ø—Ä–æ—Å–∞")
                return await analyze_competitors(payload)
            elif "[FIN]" in category or category == "FIN":
                # –î–ª—è FIN –∑–∞–ø—Ä–æ—Å–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                logger.info("‚úÖ –ó–∞–ø—Ä–æ—Å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ FIN")
                return {"category": "FIN", "message": "–ó–∞–ø—Ä–æ—Å –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Finance"}
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é. –û—Ç–≤–µ—Ç AI: {category}")
                return {"category": "UNKNOWN", "message": f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é. –û—Ç–≤–µ—Ç AI: {category}"}
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}", exc_info=True)
        return {"category": "ERROR", "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {str(e)}"}


async def analyze_competitors(payload: PromptRequest):
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —Å–æ streaming –æ—Ç–≤–µ—Ç–æ–º
    
    Args:
        payload: –ó–∞–ø—Ä–æ—Å —Å –ø—Ä–æ–º–ø—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Returns:
        StreamingResponse —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    logger.info(f"üì• –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: '{payload.prompt[:100]}...'")
    
    async def generate():
        try:
            async for chunk in analyze_competitors_streaming(payload.prompt):
                yield chunk
                await asyncio.sleep(0)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —á–∞–Ω–∫–æ–≤
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}", exc_info=True)
            yield f"\n\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/plain; charset=utf-8",
        headers={
            "Cache-Control": "no-cache, no-store, must-revalidate",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Pragma": "no-cache",
            "Expires": "0",
            "Transfer-Encoding": "chunked"
        }
    )

